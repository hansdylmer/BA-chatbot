from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter

# 游댳 1. Indl칝s dokument
with open("dit-fribeloeb-er-nedsat-naar-du-modtager-handicaptillaeg.txt", encoding="utf-8") as f:
    text = f.read()

# 游댳 2. Split dokument i bidder
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.create_documents([text])

# 游댳 3. Embedding
embedder = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
db = FAISS.from_documents(docs, embedder)
retriever = db.as_retriever()

# 游댳 4. LLM (lokal model)
model_name = "KennethTM/gpt2-medium-danish"  # eller en bedre dansk/multilingual model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=300)
llm = HuggingFacePipeline(pipeline=pipe)

# 游댳 5. Stil et sp칮rgsm친l og hent kontekst manuelt
question = "Hvorn친r bliver fribel칮bet nedsat?"
docs = retriever.get_relevant_documents(question)

# 游댳 6. Byg en dansk prompt manuelt
context = "\n\n".join([d.page_content for d in docs])
prompt = f"""Svar p친 sp칮rgsm친let p친 baggrund af den f칮lgende tekst. Hvis du ikke kan finde svaret, s친 sig "det fremg친r ikke".

### Tekst:
{context}

### Sp칮rgsm친l:
{question}

### Svar:
"""

# 游댳 7. Gener칠r svar
response = llm(prompt)
print("游 Svar:", response)
